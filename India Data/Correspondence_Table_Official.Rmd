---
title: "Corp Table"
author: "Jobs Group"
date: "2024-04-19"
output: html_document
---

```{r}
library(haven)
data <- read_dta("C:/Users/benja/Downloads/IND_2009_EUS_V01_M_V06_A_GLD_ALL.dta")
```

```{r}
library(haven)
data2 <- read_dta("C:/Users/benja/Downloads/IND_2011_EUS_V01_M_V06_A_GLD_ALL.dta")
```

Load in the correspondence table; the code below creates a correspondence table between ISIC 4 and ISIC 3.1
```{r}
library(stringr)
#setwd("~/Desktop/econo2")
correspondence <- read.csv("ISIC_words.txt", header = TRUE, stringsAsFactors = FALSE)
```

Drops the detail column, aggregates the data based on ISIC3.1 code
```{r}
correspondence <- correspondence[, !(names(correspondence) == "Detail")]

grouped_data <- split(correspondence$ISIC31code, correspondence$ISIC4code)

result_df <- data.frame(ISIC4code = character(), ISIC31code = character(), stringsAsFactors = FALSE)

for (code in names(grouped_data)) {
  isic31_codes <- unique(grouped_data[[code]])
  
  for (isic31_code in isic31_codes) {
    result_df <- rbind(result_df, data.frame(ISIC4code = code, ISIC31code = isic31_code))
  }
}

result_df$ISIC4code <- str_pad(result_df$ISIC4code, width = 4, side = "left", pad = "0")

result_df$ISIC31code <- str_pad(result_df$ISIC31code, width = 4, side = "left", pad = "0")

```

Creates the correspondence table
```{r}

get_correspondence_table <- function(correspondence_data) {
  agg_data <- aggregate(ISIC31code ~ ISIC4code, data = correspondence_data, FUN = unique)
  
  return(agg_data)
}

# For some reason R drops the 0 in front of codes like 0111, so this restores them
correspondence_table <- get_correspondence_table(result_df)
correspondence_table$ISIC4code <- sprintf("%s", correspondence_table$ISIC4code)


```

Creates a new dataframe with the 2011 ISIC code data to make prediction table; drops empty entries.
```{r}
ISIC4 <- data2$industrycat_isic_year

ISIC4[ISIC4 == ""] <- NA
ISIC4 <- na.omit(ISIC4)

df <- data.frame(ISIC4 =ISIC4, ISIC31 = NA)
# ISIC31 is empty so it can be populated in the next step
```

Matches Indian ISIC codes with the corresponding ones in 3.1.
```{r}

df$ISIC4 <- as.character(df$ISIC4)
correspondence_table$ISIC4code <- as.character(correspondence_table$ISIC4code)

for (i in seq_along(df$ISIC4)) {
  code <- df$ISIC4[i]
  match <- correspondence_table$ISIC4code == code
  if (any(match)) {
    df$ISIC31[i] <- correspondence_table$ISIC31code[match]
  }
}


```

Goes through each option and selects one for a new column, called selected_options, also takes probabilities and adds them to a new column as well. Prediction table!
```{r}
select_option_with_probability <- function(options) {
  if (is.vector(options)) {
    selected_option <- sample(options, 1)
    
    probability <- 1 / length(options)
    
    return(list(selected_option = selected_option, probability = probability))
  } else if (is.list(options)) {
    selected_option <- sample(options, 1)
    
    probability <- 1 / length(options)
    
    return(list(selected_option = selected_option, probability = probability))
  } else {
    return(NULL)
  }
}

selected_options <- lapply(df$ISIC31, select_option_with_probability)

df$selected_option <- sapply(selected_options, function(x) x$selected_option)
df$probability <- sapply(selected_options, function(x) x$probability)

print(df)
```
Start of constructing the correspondence table

```{r}
num_unique_isic4 <- length(unique(df$ISIC4))
print(paste("Number of unique ISIC4 codes:", num_unique_isic4))
```

This seperates both df into 2 df 1 is where prob is equal to 1 and df is otherwise
```{r}
# create a new dataframe with rows where 'probability' is 1
df1 <- df[df$probability == 1, ]

# remove the rows from the original dataframe where 'probability' is 1
df <- df[df$probability != 1, ]


print(head(df1))
print(head(df))

```
Create the weighted probabilities using the sum of job codes and proportions based on their mapping
```{r}
library(dplyr)
# new function to split the 'ISIC31' column and count the frequency of 'selected_option' to get our mapping
calculate_probabilities <- function(df) {
  # splitting the 'ISIC31' strings into separate rows
  split_df <- tidyr::separate_rows(df, ISIC31, sep = ",")
  
  # removing quotes and extra spaces from 'ISIC31' to clean up the data
  split_df$ISIC31 <- gsub('["c()]', '', split_df$ISIC31)
  split_df$ISIC31 <- trimws(split_df$ISIC31)
  
  # calculate the frequency of 'selected_option' corresponding to 'ISIC4'
  freq_df <- dplyr::count(split_df, ISIC4, ISIC31, selected_option)
  
  # calculate probabilities
  freq_df <- freq_df %>%
    group_by(ISIC4, ISIC31) %>%
    mutate(probability = n / sum(n))
  
  # get the correct columns for new df
  df2 <- freq_df %>% 
    select(ISIC4, ISIC31, probability) %>%
    distinct()
  
  return(df2)
}

df2 <- calculate_probabilities(df)


print(head(df2))

```

Cleaning up the df to better represent the selected option in the mapping
```{r}
library(dplyr)
library(tidyr)

# clean the 'ISIC31' column by removing the 'c()', quotes, and splitting it into separate rows for each code
df <- df %>% 
  mutate(ISIC31 = gsub("c\\(|\\)|\"", "", ISIC31)) %>%
  separate_rows(ISIC31, sep = ",\\s*")

# get the frequency for ecah selected option in 4.0
df2 <- df %>%
  group_by(ISIC4, ISIC31, selected_option) %>%
  summarise(count = n(), .groups = "drop") %>%
  mutate(probability = count / sum(count)) %>%
  ungroup() %>%
  filter(ISIC31 == selected_option)  # making sure ISIC31 matches the selected_option

print(head(df2))

```
The final product for now is df3 as the correspondence table
```{r}
df3 <- df2 %>%
  group_by(ISIC4) %>%
  mutate(probability = count / sum(count)) %>%
  ungroup() %>%
  select(ISIC4, ISIC31, selected_option, probability) 

# Remove the column 'selected_option' from df3
df3 <- df3[ , !(names(df3) %in% c("selected_option"))]

# View the updated dataframe
print(df3)

print(head(df3))
print(df3)
```

Now we read in the tools results 
```{r}
library(readxl)

# Specify the path to your Excel file
file_path <- "C:/users/benja/Downloads/correspondence_table.xlsx"
df6 <- read_excel(file_path)

library(dplyr)

# Assuming the second dataframe is named df2 and the first dataframe's column names are as shown
df6 <- df6 %>% rename(
  ISIC4 = version_4,
  ISIC31 = version_3.1,
  probability = `Proportion of Jobs`
)

# Print the renamed dataframe
print(df6)
```
The probaility differences between the results and the obersved worked, but 141 missing NA values is pretty high, so going to retry without the proabilites of 1. df10 is just df3 again with no probailites of 1 from the India data.
```{r}
df10 <- df2 %>%
  group_by(ISIC4) %>%
  mutate(probability = count / sum(count)) %>%
  ungroup() %>%
  select(ISIC4, ISIC31, selected_option, probability) 

# Remove the column 'selected_option' from df3
df10 <- df10[ , !(names(df10) %in% c("selected_option"))]

# View the updated dataframe
print(df10)
```
df11 is the tools result without the probalities of 1
```{r}
# Drop all rows where probability is 1
df11 <- subset(df6, probability != 1)
print(df11)
```
df12 contains the final proability differences between the results and India occupation codes with only 47 missing values 
```{r}
library(dplyr)

# Create a key for matching
df11$key <- paste(df11$ISIC4, df11$ISIC31, sep = "_")
df10$key <- paste(df10$ISIC4, df10$ISIC31, sep = "_")

# Merge df11 and df10 to align the rows
merged_df <- merge(df11, df10, by = "key", suffixes = c("_df11", "_df10"), all.x = TRUE)

# Calculate the difference between the probabilities
merged_df$prob_diff <- merged_df$probability_df11 - merged_df$probability_df10

# Create the new dataframe df12 with the required columns and filter out missing values
df12 <- merged_df %>%
  filter(!is.na(probability_df10)) %>%
  select(ISIC4 = ISIC4_df11, ISIC31 = ISIC31_df11, prob_diff)

# Set scipen option to remove scientific notation and round to two decimal places
options(scipen = 999)
df12$prob_diff <- round(df12$prob_diff, 2)

# Remove the key columns
df11$key <- NULL
df10$key <- NULL

print("df12:")
print(df12)
```

Here are the missing values from the probaility differences 
```{r}
# Separate rows where df10 has missing values
df_missing <- merged_df %>% filter(is.na(probability_df10))

print("df_missing:")
print(df_missing)
```

Now lets do the general case
```{r}
library(dplyr)

# Calculate the number of unique ISIC31 codes for each ISIC4
df_general <- df11 %>%
  group_by(ISIC4) %>%
  mutate(probability = 1 / n()) %>%
  ungroup() %>%

# Print the new dataframe
print(df_general)

```

The differences between the proabilites in the general case and the tools results 
```{r}
library(dplyr)

# Create the df_gen_diff dataframe with the probability differences and round to two decimal places
df_gen_diff <- df11 %>%
  mutate(probability_general = df_general$probability,
         prob_diff = round(probability - probability_general, 2)) %>%
  select(ISIC4, ISIC31, prob_diff)

# Print the new dataframe
print("df_gen_diff:")
print(df_gen_diff)

```
Differences between general case and the india occupation data
```{r}
library(dplyr)

# Create a key for matching
df10 <- df10 %>% mutate(key = paste(ISIC4, ISIC31, sep = "_"))
df_general <- df_general %>% mutate(key = paste(ISIC4, ISIC31, sep = "_"))

# Merge df10 and df_general to align the rows
merged_df <- merge(df10, df_general, by = "key", suffixes = c("_df10", "_df_general"), all.x = TRUE)

# Calculate the difference between the probabilities
merged_df <- merged_df %>% mutate(prob_diff = probability_df10 - probability_df_general)

# Create the new dataframe df_gen_india with the required columns and filter out missing values
df_gen_india <- merged_df %>%
  filter(!is.na(probability_df_general)) %>%
  select(ISIC4 = ISIC4_df10, ISIC31 = ISIC31_df10, prob_diff)

# Round prob_diff to two decimal places
df_gen_india <- df_gen_india %>% mutate(prob_diff = round(prob_diff, 2))

# Remove the key columns from the original dataframes
df10 <- df10 %>% select(-key)
df_general <- df_general %>% select(-key)

print("df_gen_india:")
print(df_gen_india)

```
```{r}
library(ggplot2)

# Create bins for prob_diff in intervals of 0.10 without double counting
df12 <- df12 %>%
  mutate(prob_diff_bin = cut(prob_diff, breaks = seq(-1, 1, by = 0.10), right = FALSE, include.lowest = TRUE))

# Count the occurrences in each bin
prob_diff_counts <- df12 %>%
  group_by(prob_diff_bin) %>%
  summarise(count = n()) %>%
  arrange(prob_diff_bin)

# Print the counts
print(prob_diff_counts)

bin_colors <- ifelse(grepl("-", prob_diff_counts$prob_diff_bin), "red", "blue")

# Create a bar chart with different shades for negative and positive values
#negative is underestimated and positive is overestimated
ggplot(prob_diff_counts, aes(x = reorder(prob_diff_bin, desc(prob_diff_bin)), y = count)) +
  geom_bar(stat = "identity", aes(fill = prob_diff_bin)) +
  geom_text(aes(label = count), hjust = -0.1) + # Add count labels at the end of the bars
  scale_fill_manual(values = bin_colors) +
  coord_flip() +
  theme_minimal() +
  labs(title = "Count of Probability Differences in Intervals of 0.10",
       x = "Probability Difference Bins",
       y = "Count") +
  theme(axis.text.x = element_text(angle = 0, hjust = 1), legend.position = "none")

```
```{r}
# Create the second dataframe (df_2digit_2009)
df_2digit_2009 <- data.frame(
  Activity = c("Activities of private households",
               "Agriculture, hunting and forestry",
               "Construction",
               "Education",
               "Electricity, gas and water supply",
               "Extraterritorial organizations and bodies",
               "Financial intermediation",
               "Fishing",
               "Health and social work",
               "Hotels and restaurants",
               "Manufacturing",
               "Mining and quarrying",
               "Other community, social and personal service activities",
               "Public administration and defence",
               "Real estate, renting and business activities",
               "Transport, storage and communications",
               "Wholesale and retail trade"),
  Freq = c(1637, 67134, 18827, 8109, 857, 1, 1815, 627, 2127, 3115, 20452, 1221, 4336, 7701, 2420, 9292, 23663)
)

# Print the second dataframe
print(df_2digit_2009)

```
```{r}
# Create the updated dataframe (df_2digit_2011)
df_2digit_2011 <- data.frame(
  Category = c("Activities of private households",
               "Agriculture, hunting and forestry",
               "Construction",
               "Education",
               "Electricity, gas and water supply",
               "Extraterritorial organizations and bodies",
               "Financial intermediation",
               "Fishing",
               "Health and social work",
               "Hotels and restaurants",
               "Manufacturing",
               "Mining and quarrying",
               "Other community, social and personal service activities",
               "Public administration and defence",
               "Real estate, renting and business activities",
               "Transport, storage and communications",
               "Wholesale and retail trade"),
  Freq = c(1561, 61573, 20227, 9170, 997, 1, 1943, 499, 2440, 3543, 21735, 972, 5899, 6627, 2988, 8361, 23750)
)

# Calculate the total frequency
total_freq_2011 <- sum(df_2digit_2011$Freq)

# Calculate the percentage for each group
df_2digit_2011 <- df_2digit_2011 %>%
  mutate(Percentage = (Freq / total_freq_2011) * 100)

# Print the resulting dataframe
print(df_2digit_2011)

```

```{r}
# Calculate the total frequency
total_freq_2009 <- sum(df_2digit_2009$Freq)

# Calculate the percentage for each group
df_2digit_2009 <- df_2digit_2009 %>%
  mutate(Percentage = (Freq / total_freq_2009) * 100)

# Print the resulting dataframe
print(df_2digit_2009)

```

```{r}
# Create the dataframe df_2digit_blunt
df_2digit_blunt <- data.frame(
  Category = c("Activities of private households",
               "Agriculture, hunting and forestry",
               "Construction",
               "Education",
               "Electricity, gas and water supply",
               "Extraterritorial organizations and bodies",
               "Financial intermediation",
               "Fishing",
               "Health and social work",
               "Hotels and restaurants",
               "Manufacturing",
               "Mining and quarrying",
               "Other community, social and personal service activities",
               "Public administration and defence",
               "Real estate, renting and business activities",
               "Transport, storage and communications",
               "Wholesale and retail trade"),
  Freq = c(1561, 61581, 20234, 9150, 997, 1, 1937, 504, 2440, 3543, 21743, 969, 5935, 6624, 2975, 8337, 23750)
)

# Print the resulting dataframe
print(df_2digit_blunt)

```

```{r}
# Remove the Percentage column from df_2digit_2011
df_2digit_2011 <- df_2digit_2011[, c("Category", "Freq")]

# Combine the dataframes
df_combined <- rbind(
  transform(df_2digit_2011, Dataset = "AI"),
  transform(df_2digit_blunt, Dataset = "Blunt")
)

# Reorder Category to be alphabetical from A to Z with A at the top
df_combined$Category <- factor(df_combined$Category, levels = rev(sort(unique(df_combined$Category))))

# Load necessary library
library(ggplot2)

# Plot the comparison graph
ggplot(df_combined, aes(x = Category, y = Freq, fill = Dataset)) +
  geom_bar(stat = "identity", position = "dodge") +
  coord_flip() +
  labs(title = "Frequency of Categories: AI vs Blunt",
       x = "Category",
       y = "Frequency") +
  theme_minimal() +
  scale_fill_manual(values = c("AI" = "blue", "Blunt" = "red"))

```

